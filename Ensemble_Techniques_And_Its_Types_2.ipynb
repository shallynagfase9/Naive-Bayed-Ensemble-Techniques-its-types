{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPyikyQcTp6X/NQhZ0ezfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/Naive-Bayes-Ensemble-Techniques-its-types/blob/main/Ensemble_Techniques_And_Its_Types_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. How does bagging reduce overfitting in decision trees?"
      ],
      "metadata": {
        "id": "D13evvVyUFwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bagging, or Bootstrap Aggregating, is an ensemble technique that helps reduce overfitting in decision trees by combining the predictions of multiple models.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "F8l3hAnbUH3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
      ],
      "metadata": {
        "id": "hkR-9l2UUIBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The choice of base learner in bagging depends on the specific problem, dataset characteristics, and computational resources. Decision trees are commonly used due to their high variance, which bagging effectively reduces.\n",
        "However, depending on the problem, other base learners like linear models, KNN, SVMs, or neural networks can be used to create a robust ensemble with different strengths and weaknesses.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_2G-8ZqyUJq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
      ],
      "metadata": {
        "id": "wga84aIBUJyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The effectiveness of bagging depends heavily on the properties of the base learners.\n",
        "High-variance learners like decision trees benefit the most from bagging due to significant variance reduction, leading to improved generalization without a substantial increase in bias.\n",
        "Low-variance learners like linear models see less benefit since their variance is already low, and bagging does not reduce bias.\n",
        "Flexible learners like neural networks and non-parametric methods like KNN can also benefit from reduced variance, but the overall effect depends on their initial bias-variance characteristics.\n",
        "\n",
        "Effects on Bias-Variance Tradeoff-\n",
        "\n",
        "Decision Trees (High-Variance Learners):\n",
        "Bias: Low (remains low after bagging)\n",
        "Variance: High (significantly reduced by bagging)\n",
        "\n",
        "Linear Models (Low-Variance Learners):\n",
        "Bias: High (remains high after bagging)\n",
        "Variance: Low (slightly reduced by bagging, but less impact)\n",
        "\n",
        "Neural Networks (Flexible Learners):\n",
        "Bias: Variable (depends on architecture; bagging doesn't significantly change bias)\n",
        "Variance: High (reduced by bagging)\n",
        "\n",
        "K-Nearest Neighbors (Non-parametric Methods):\n",
        "Bias: Variable (depends on k; bagging doesn't change bias)\n",
        "Variance: High (for low k; reduced by bagging)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Xph3Y0x0ULV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
      ],
      "metadata": {
        "id": "93w5h_L6ULdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bagging is a flexible and effective ensemble technique that can be applied to both classification and regression tasks.\n",
        "While the core idea remains consistent—training multiple models on different subsets of data and aggregating their predictions—the specific implementation details, choice of base learners, and methods for prediction aggregation vary between\n",
        "classification and regression scenarios to best suit the nature of the target variable and the goals of the modeling task.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YJ9b-Z5xUM5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
      ],
      "metadata": {
        "id": "YIWQcgmlUNEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The ensemble size in bagging refers to the number of base models (often referred to as base learners) that are trained and aggregated to form the final ensemble prediction.\n",
        "The choice of ensemble size is an important factor in determining the effectiveness and performance of bagging.\n",
        "\n",
        "\n",
        "The optimal number of models in the ensemble depends on several factors, including:\n",
        "\n",
        "Dataset Size:\n",
        "Larger datasets can generally support larger ensembles without overfitting. For smaller datasets, a smaller ensemble size may be more appropriate to avoid overfitting.\n",
        "\n",
        "Model Complexity:\n",
        "More complex models (e.g., deep neural networks) might benefit from larger ensembles to reduce variance, whereas simpler models (e.g., decision trees) may achieve sufficient performance with a smaller ensemble.\n",
        "\n",
        "Computational Constraints:\n",
        "Practical considerations such as computational resources, training time, and memory limitations may restrict the size of the ensemble that can be feasibly trained and deployed.\n",
        "\n",
        "Cross-validation and Validation Performance:\n",
        "Cross-validation or validation performance metrics can help determine the optimal ensemble size. It’s common to monitor performance metrics (e.g., accuracy, MSE) on a validation set or through cross-validation to assess how adding more models impacts performance.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jT-QubrHUPLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Can you provide an example of a real-world application of bagging in machine learning?"
      ],
      "metadata": {
        "id": "td16MUvqUPUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In this medical diagnosis example, bagging with decision trees (Random Forest) serves as a powerful tool for building a robust and accurate predictive model.\n",
        "The ensemble technique leverages the strengths of decision trees while mitigating their weaknesses, ultimately aiding healthcare professionals in making more informed diagnostic decisions based on patient data.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BNox3ULVURIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}