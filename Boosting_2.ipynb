{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0gXOyqgLoAw3ePYUhng/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/Naive-Bayes-Ensemble-Techniques-its-types/blob/main/Boosting_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1. What is Gradient Boosting Regression?"
      ],
      "metadata": {
        "id": "wQq-Ixn_nRW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Gradient Boosting Regression is a machine learning technique used for regression tasks that involves building an ensemble of decision trees sequentially.\n",
        "It belongs to the family of boosting algorithms and is particularly effective in creating predictive models where the goal is to predict a continuous numeric value (regression).\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R9ZnEORsnTG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
      ],
      "metadata": {
        "id": "zAxIbOubnTa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        if depth >= self.max_depth:\n",
        "            return np.mean(y)  # leaf node value is mean of y\n",
        "\n",
        "        feature_index, threshold, gain = self._find_best_split(X, y)\n",
        "\n",
        "        if gain == 0:\n",
        "            return np.mean(y)  # if no gain, return mean of y for leaf node\n",
        "\n",
        "        left_indices = X[:, feature_index] < threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return {'feature_index': feature_index,\n",
        "                'threshold': threshold,\n",
        "                'left_subtree': left_subtree,\n",
        "                'right_subtree': right_subtree}\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        best_feature_index = None\n",
        "        best_threshold = None\n",
        "        best_gain = 0\n",
        "\n",
        "        m, n = X.shape\n",
        "        base_error = np.var(y)\n",
        "\n",
        "        for feature_index in range(n):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature_index] < threshold\n",
        "                right_indices = ~left_indices\n",
        "\n",
        "                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                left_variance = np.var(y[left_indices])\n",
        "                right_variance = np.var(y[right_indices])\n",
        "\n",
        "                weighted_variance = (np.sum(left_indices) / m) * left_variance + \\\n",
        "                                    (np.sum(right_indices) / m) * right_variance\n",
        "\n",
        "                gain = base_error - weighted_variance\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature_index, best_threshold, best_gain\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict_tree(self, x, tree):\n",
        "        if isinstance(tree, dict):\n",
        "            feature_index = tree['feature_index']\n",
        "            threshold = tree['threshold']\n",
        "\n",
        "            if x[feature_index] < threshold:\n",
        "                return self._predict_tree(x, tree['left_subtree'])\n",
        "            else:\n",
        "                return self._predict_tree(x, tree['right_subtree'])\n",
        "        else:\n",
        "            return tree\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.estimators = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_pred = np.full_like(y, np.mean(y))  # initialize predictions with mean of y\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.estimators.append(tree)\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(len(X))\n",
        "        for estimator in self.estimators:\n",
        "            y_pred += self.learning_rate * estimator.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mse, r2\n",
        "\n",
        "# Example usage with a simple regression dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X[:, 0] + np.random.randn(100)  # linear relationship with noise\n",
        "\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=2)\n",
        "gb_regressor.fit(X, y)\n",
        "\n",
        "y_pred = gb_regressor.predict(X)\n",
        "\n",
        "mse, r2 = evaluate_performance(y, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uSNpdMYnW3v",
        "outputId": "6cc09b3e-f2ee-42e5-b574-c25350be9def"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 88.5412\n",
            "R-squared: -1.5844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
      ],
      "metadata": {
        "id": "m7g0OsSQnXQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        if depth >= self.max_depth:\n",
        "            return np.mean(y)  # leaf node value is mean of y\n",
        "\n",
        "        feature_index, threshold, gain = self._find_best_split(X, y)\n",
        "\n",
        "        if gain == 0:\n",
        "            return np.mean(y)  # if no gain, return mean of y for leaf node\n",
        "\n",
        "        left_indices = X[:, feature_index] < threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return {'feature_index': feature_index,\n",
        "                'threshold': threshold,\n",
        "                'left_subtree': left_subtree,\n",
        "                'right_subtree': right_subtree}\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        best_feature_index = None\n",
        "        best_threshold = None\n",
        "        best_gain = 0\n",
        "\n",
        "        m, n = X.shape\n",
        "        base_error = np.var(y)\n",
        "\n",
        "        for feature_index in range(n):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature_index] < threshold\n",
        "                right_indices = ~left_indices\n",
        "\n",
        "                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                left_variance = np.var(y[left_indices])\n",
        "                right_variance = np.var(y[right_indices])\n",
        "\n",
        "                weighted_variance = (np.sum(left_indices) / m) * left_variance + \\\n",
        "                                    (np.sum(right_indices) / m) * right_variance\n",
        "\n",
        "                gain = base_error - weighted_variance\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature_index, best_threshold, best_gain\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict_tree(self, x, tree):\n",
        "        if isinstance(tree, dict):\n",
        "            feature_index = tree['feature_index']\n",
        "            threshold = tree['threshold']\n",
        "\n",
        "            if x[feature_index] < threshold:\n",
        "                return self._predict_tree(x, tree['left_subtree'])\n",
        "            else:\n",
        "                return self._predict_tree(x, tree['right_subtree'])\n",
        "        else:\n",
        "            return tree\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.estimators = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_pred = np.full_like(y, np.mean(y))  # initialize predictions with mean of y\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.estimators.append(tree)\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(len(X))\n",
        "        for estimator in self.estimators:\n",
        "            y_pred += self.learning_rate * estimator.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mse, r2\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'max_depth': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# Initialize the GradientBoostingRegressor\n",
        "gb_regressor = GradientBoostingRegressor()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "\n"
      ],
      "metadata": {
        "id": "3KWdBLmnnbHP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4. What is a weak learner in Gradient Boosting?"
      ],
      "metadata": {
        "id": "0D_js7YMnbeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In the context of Gradient Boosting, a weak learner refers to a simple or basic model that performs slightly better than random guessing for a given learning task.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1lNh7et-ndRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5. What is the intuition behind the Gradient Boosting algorithm?"
      ],
      "metadata": {
        "id": "S6o6JqRqndav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The intuition behind the Gradient Boosting algorithm revolves around sequentially improving the performance of a model by leveraging the strengths of weak learners and correcting their errors.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_b2ZoI-AnfNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
      ],
      "metadata": {
        "id": "q5QEeKpfnfcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The Gradient Boosting algorithm builds an ensemble of weak learners, typically decision trees, in a sequential manner\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DNHkmQ6cnhnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
      ],
      "metadata": {
        "id": "3KsD9IrAnh3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "- Loss Function Definition\n",
        "- Initial Prediction\n",
        "- Residual Calculation\n",
        "- Sequential Model Training\n",
        "- Update Predictions\n",
        "- Repeat\n",
        "- Ensemble Combination\n",
        "- Regularization\n",
        "- Objective Function Optimization\n",
        "- Evaluation\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qVxhTdVlnkvQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}