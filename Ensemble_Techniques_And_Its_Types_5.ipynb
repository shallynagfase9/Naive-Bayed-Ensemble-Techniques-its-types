{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUZpdMXp2N6h848SoGGTjR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/Naive-Bayes-Ensemble-Techniques-its-types/blob/main/Ensemble_Techniques_And_Its_Types_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. You are working on a machine learning project where you have a dataset containing numerical and\n",
        "categorical features. You have identified that some of the features are highly correlated and there are\n",
        "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
        "engineering process and handles the missing values"
      ],
      "metadata": {
        "id": "lE3spFcFim2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "df = sns.load_dataset(\"tips\")\n",
        "df[\"time\"].unique()\n",
        "# LabelEncoder assigns Labels to categorical values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit_transform(df[\"time\"])\n",
        "df[\"time\"] = encoder.fit_transform(df[\"time\"])\n",
        "df[\"time\"].value_counts()\n",
        "# Independent and dependent features\n",
        "x = df.drop(labels=[\"time\"], axis = 1)\n",
        "y = df[\"time\"]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=42)\n",
        "\n",
        "X_train.head()\n",
        "from sklearn.impute import SimpleImputer ## Handling Missing Values\n",
        "from sklearn.preprocessing import OneHotEncoder## handling Categorical features\n",
        "from sklearn.preprocessing import StandardScaler## Feature scaling\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "## Automating the entire\n",
        "categorical_cols = ['sex', 'smoker','day']\n",
        "numerical_cols = ['total_bill', 'tip','size']\n",
        "\n",
        "## Feature Engineering Automation\n",
        "num_pipeline=Pipeline(\n",
        "    steps=[\n",
        "        ('imputer',SimpleImputer(strategy='median')), ##missing values\n",
        "        ('scaler',StandardScaler())## feature scaling\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "#categorical Pipeline\n",
        "cat_pipeline=Pipeline(\n",
        "                steps=[\n",
        "                ('imputer',SimpleImputer(strategy='most_frequent')), ## handling Missing values\n",
        "                ('onehotencoder',OneHotEncoder()) ## Categorical features to numerical\n",
        "                ]\n",
        "\n",
        "            )\n",
        "preprocessor=ColumnTransformer([\n",
        "    ('num_pipeline',num_pipeline,numerical_cols),\n",
        "    ('cat_pipeline',cat_pipeline,categorical_cols)\n",
        "\n",
        "])\n",
        "X_train=preprocessor.fit_transform(X_train)\n",
        "X_test=preprocessor.transform(X_test)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "## Automate Model Training Process\n",
        "models={\n",
        "    'Random Forest':RandomForestClassifier(),\n",
        "    'Decision Tree':DecisionTreeClassifier(),\n",
        "    'SVC':SVC()\n",
        "\n",
        "}\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_model(X_train,y_train,X_test,y_test,models):\n",
        "\n",
        "    report = {}\n",
        "    for i in range(len(models)):\n",
        "        model = list(models.values())[i]\n",
        "        # Train model\n",
        "        model.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "\n",
        "        # Predict Testing data\n",
        "        y_test_pred =model.predict(X_test)\n",
        "\n",
        "        # Get accuracy for test data prediction\n",
        "\n",
        "        test_model_score = accuracy_score(y_test,y_test_pred)\n",
        "\n",
        "        report[list(models.keys())[i]] =  test_model_score\n",
        "\n",
        "\n",
        "\n",
        "    return report\n",
        "\n",
        "evaluate_model(X_train,y_train,X_test,y_test,models)\n",
        "classifier=RandomForestClassifier()\n",
        "\n",
        "## Hypeparameter Tuning\n",
        "params={'max_depth':[3,5,10,None],\n",
        "              'n_estimators':[100,200,300],\n",
        "               'criterion':['gini','entropy']\n",
        "              }\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "cv=RandomizedSearchCV(classifier,param_distributions=params,scoring='accuracy',cv=5,verbose=3)\n",
        "cv.fit(X_train,y_train)\n",
        "\n",
        "cv.best_params_"
      ],
      "metadata": {
        "id": "hDH3lovoi6xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e56607e-2cc6-4767-ce8b-578605c0def4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "[CV 1/5] END criterion=entropy, max_depth=5, n_estimators=200;, score=0.974 total time=   0.3s\n",
            "[CV 2/5] END criterion=entropy, max_depth=5, n_estimators=200;, score=0.923 total time=   0.3s\n",
            "[CV 3/5] END criterion=entropy, max_depth=5, n_estimators=200;, score=0.974 total time=   0.3s\n",
            "[CV 4/5] END criterion=entropy, max_depth=5, n_estimators=200;, score=0.949 total time=   0.3s\n",
            "[CV 5/5] END criterion=entropy, max_depth=5, n_estimators=200;, score=0.949 total time=   0.3s\n",
            "[CV 1/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.974 total time=   0.3s\n",
            "[CV 2/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.923 total time=   0.5s\n",
            "[CV 3/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.974 total time=   0.5s\n",
            "[CV 4/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.923 total time=   0.5s\n",
            "[CV 5/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.949 total time=   0.5s\n",
            "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=0.974 total time=   0.3s\n",
            "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=0.923 total time=   0.3s\n",
            "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=1.000 total time=   0.3s\n",
            "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=0.949 total time=   0.3s\n",
            "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=0.923 total time=   0.3s\n",
            "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=200;, score=0.974 total time=   0.4s\n",
            "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=200;, score=0.923 total time=   0.3s\n",
            "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=200;, score=1.000 total time=   0.3s\n",
            "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=200;, score=0.923 total time=   0.3s\n",
            "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=200;, score=0.923 total time=   0.3s\n",
            "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=0.974 total time=   0.4s\n",
            "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=0.923 total time=   0.5s\n",
            "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=1.000 total time=   0.4s\n",
            "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=0.949 total time=   0.5s\n",
            "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=0.923 total time=   0.5s\n",
            "[CV 1/5] END criterion=gini, max_depth=3, n_estimators=300;, score=0.974 total time=   0.5s\n",
            "[CV 2/5] END criterion=gini, max_depth=3, n_estimators=300;, score=0.949 total time=   0.4s\n",
            "[CV 3/5] END criterion=gini, max_depth=3, n_estimators=300;, score=0.974 total time=   0.5s\n",
            "[CV 4/5] END criterion=gini, max_depth=3, n_estimators=300;, score=0.923 total time=   0.4s\n",
            "[CV 5/5] END criterion=gini, max_depth=3, n_estimators=300;, score=0.923 total time=   0.4s\n",
            "[CV 1/5] END criterion=gini, max_depth=3, n_estimators=200;, score=0.974 total time=   0.3s\n",
            "[CV 2/5] END criterion=gini, max_depth=3, n_estimators=200;, score=0.974 total time=   0.3s\n",
            "[CV 3/5] END criterion=gini, max_depth=3, n_estimators=200;, score=0.974 total time=   0.3s\n",
            "[CV 4/5] END criterion=gini, max_depth=3, n_estimators=200;, score=0.923 total time=   0.3s\n",
            "[CV 5/5] END criterion=gini, max_depth=3, n_estimators=200;, score=0.949 total time=   0.3s\n",
            "[CV 1/5] END criterion=gini, max_depth=10, n_estimators=100;, score=0.974 total time=   0.2s\n",
            "[CV 2/5] END criterion=gini, max_depth=10, n_estimators=100;, score=0.897 total time=   0.2s\n",
            "[CV 3/5] END criterion=gini, max_depth=10, n_estimators=100;, score=1.000 total time=   0.1s\n",
            "[CV 4/5] END criterion=gini, max_depth=10, n_estimators=100;, score=0.949 total time=   0.2s\n",
            "[CV 5/5] END criterion=gini, max_depth=10, n_estimators=100;, score=0.949 total time=   0.2s\n",
            "[CV 1/5] END criterion=gini, max_depth=5, n_estimators=100;, score=0.974 total time=   0.2s\n",
            "[CV 2/5] END criterion=gini, max_depth=5, n_estimators=100;, score=0.923 total time=   0.2s\n",
            "[CV 3/5] END criterion=gini, max_depth=5, n_estimators=100;, score=0.974 total time=   0.2s\n",
            "[CV 4/5] END criterion=gini, max_depth=5, n_estimators=100;, score=0.923 total time=   0.1s\n",
            "[CV 5/5] END criterion=gini, max_depth=5, n_estimators=100;, score=0.949 total time=   0.2s\n",
            "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.974 total time=   0.2s\n",
            "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.923 total time=   0.2s\n",
            "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.974 total time=   0.1s\n",
            "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.923 total time=   0.2s\n",
            "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.923 total time=   0.2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_estimators': 200, 'max_depth': 3, 'criterion': 'gini'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
        "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "VGyz43FliarW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X=iris.data\n",
        "y=iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Define the classifiers\n",
        "rf_classifier = RandomForestClassifier(random_state=0)\n",
        "lr_classifier = LogisticRegression(random_state=0)\n",
        "# Build the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('rf', rf_classifier),\n",
        "    ('lr', lr_classifier),\n",
        "    ('voting', VotingClassifier(estimators=[('rf', rf_classifier), ('lr', lr_classifier)], voting='soft'))\n",
        "])\n",
        "\"\"\"\n",
        "# Train the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "# Predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Sn1fASeDicek",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d530ddad-44bd-4101-d7f5-93ca79ffb870"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Train the pipeline\\npipeline.fit(X_train, y_train)\\n# Predictions\\ny_pred = pipeline.predict(X_test)\\n\\n# Calculate accuracy\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(f'Accuracy: {accuracy:.4f}')\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}