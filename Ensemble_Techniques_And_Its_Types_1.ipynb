{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnsn31AxJsQt1SxN/CPFMn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/Naive-Bayes-Ensemble-Techniques-its-types/blob/main/Ensemble_Techniques_And_Its_Types_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is an ensemble technique in machine learning?"
      ],
      "metadata": {
        "id": "Lb_4CfdWQDTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "An ensemble technique in machine learning is a method that combines multiple models to improve the overall performance, accuracy, and robustness of the predictions compared to any individual model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-Cv0Kc_tQDo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Why are ensemble techniques used in machine learning?"
      ],
      "metadata": {
        "id": "TRqnnbMaQD4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ensemble techniques are a powerful tool in the machine learning toolkit, providing enhanced performance, robustness, and versatility across various applications and datasets.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ok63R249QIfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is bagging?"
      ],
      "metadata": {
        "id": "ZwqMVh2KQIx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "- How it works: Multiple models (often the same type, like decision trees) are trained on different random subsets of the training data, created by sampling with replacement. The predictions from all models are then averaged (for regression) or voted upon (for classification) to produce the final prediction.\n",
        "- Example: Random Forests.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "NhbC_adYQKN2",
        "outputId": "50171507-5c11-40d5-95f0-df07e31252dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nBagging (Bootstrap Aggregating):\\n\\n- How it works: Multiple models (often the same type, like decision trees) are trained on different random subsets of the training data, created by sampling with replacement. The predictions from all models are then averaged (for regression) or voted upon (for classification) to produce the final prediction.\\n- Example: Random Forests.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is boosting?"
      ],
      "metadata": {
        "id": "G_ljrTf2QRJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Boosting:\n",
        "\n",
        "- How it works: Models are trained sequentially, with each new model focusing on correcting the errors made by the previous models. The models are then combined to make the final prediction.\n",
        "- Example: AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XluV_BhuQS54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are the benefits of using ensemble techniques?"
      ],
      "metadata": {
        "id": "nUH2gyOcQVw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ensemble techniques, offer a powerful approach to improving the performance, robustness, and versatility of machine learning models, making them a critical tool in the data scientist's toolkit.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rwAojhhCQWSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Are ensemble techniques always better than individual models?"
      ],
      "metadata": {
        "id": "ifQJjyDVQZwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ensemble techniques are not always better than individual models. While they often provide significant advantages, there are certain scenarios where they may not be the optimal choice. Here are some considerations:\n",
        "\n",
        "Advantages of Ensemble Techniques\n",
        "- Improved Accuracy: Ensembles generally provide higher accuracy by combining the strengths of multiple models, especially when the individual models have complementary strengths and weaknesses.\n",
        "- Reduction in Overfitting: By averaging the predictions of multiple models, ensembles can help reduce the risk of overfitting, making them more robust and generalizable to new data.\n",
        "- Error Reduction: Different models make different errors. Ensemble techniques can mitigate the impact of these errors, leading to more reliable predictions.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PYEvreR5QbRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How is the confidence interval calculated using bootstrap?"
      ],
      "metadata": {
        "id": "kqa2SHd1QbgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bootstrap is a powerful statistical technique used to estimate the distribution of a statistic (such as the mean, median, variance, etc.) by resampling with replacement from the original dataset.\n",
        "This method can also be used to calculate confidence intervals for the statistic of interest.\n",
        "Here's a step-by-step guide on how to calculate the confidence interval using the bootstrap method:\n",
        "- Original Sample\n",
        "- Resampling\n",
        "- Statistic Calculation\n",
        "- Sorting\n",
        "- Confidence Interval\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HJyUHbOSQdAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
      ],
      "metadata": {
        "id": "GBOnAMymQdRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bootstrap is a resampling technique used to estimate the distribution of a statistic by repeatedly sampling with replacement from the original dataset. It is particularly useful for assessing the variability and constructing confidence intervals for statistics when the theoretical distribution is complex or unknown. Here are the steps involved in the bootstrap method:\n",
        "\n",
        "How Bootstrap Works:\n",
        "- Original Sample\n",
        "- Resampling\n",
        "- Statistic Calculation\n",
        "- Estimation\n",
        "\n",
        "Steps Involved in Bootstrap:\n",
        "- Obtain the Original Sample\n",
        "- Generate Bootstrap Samples\n",
        "- Calculate the Statistic for Each Bootstrap Sample\n",
        "- Create the Bootstrap Distribution\n",
        "- Estimate the Standard Error\n",
        "- Construct Confidence Intervals\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jFhqFsagQeq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
        "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
        "bootstrap to estimate the 95% confidence interval for the population mean height."
      ],
      "metadata": {
        "id": "4_r4rvzQQe31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given statistics\n",
        "sample_mean = 15\n",
        "sample_std = 2\n",
        "n = 50\n",
        "B = 1000  # Number of bootstrap samples\n",
        "\n",
        "# Generate a synthetic dataset based on the given sample mean and std deviation\n",
        "np.random.seed(42)  # For reproducibility\n",
        "original_sample = np.random.normal(sample_mean, sample_std, n)\n",
        "\n",
        "# Generate bootstrap samples and calculate the mean for each sample\n",
        "bootstrap_means = np.zeros(B)\n",
        "for i in range(B):\n",
        "    bootstrap_sample = np.random.choice(original_sample, size=n, replace=True)\n",
        "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
        "\n",
        "# Sort the bootstrap means to find percentiles\n",
        "sorted_means = np.sort(bootstrap_means)\n",
        "\n",
        "# Calculate the 95% confidence interval (2.5th and 97.5th percentiles)\n",
        "alpha = 0.05\n",
        "lower_bound = np.percentile(sorted_means, 100 * (alpha / 2))\n",
        "upper_bound = np.percentile(sorted_means, 100 * (1 - alpha / 2))\n",
        "\n",
        "# Output the results\n",
        "print(f\"95% Confidence Interval for the mean height: [{lower_bound:.2f}, {upper_bound:.2f}] meters\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9wnMG40QhXF",
        "outputId": "63293583-86fc-4564-8175-7c16e3a542cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% Confidence Interval for the mean height: [14.03, 15.09] meters\n"
          ]
        }
      ]
    }
  ]
}